{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_NN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "x9TyZ_wJafwd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import random\n",
        "from time import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1hyplUw5bEQr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Constants:\n",
        "  \n",
        "  SIGMOID = \"sigmoid\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ZvmNBiwWy0c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neuron:\n",
        "  \n",
        "  # id_ = layer.neuron_num\n",
        "  id_ = None\n",
        "  # name of the activation function\n",
        "  act_func = None \n",
        "  in_edges = None\n",
        "  out_edges = None\n",
        "  # output of a neuron i.e activation(summation(w_i*x_i))\n",
        "  output = None       \n",
        "  # error at a neuron during back propagation\n",
        "  local_error = None  \n",
        "  # input to the input-layer-neuron\n",
        "  input_datum = None   \n",
        "  # derivative of loss; loss = softmax and then cross entropy\n",
        "  loss_der = None   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4zEiitEIeV4Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neuron(Neuron):\n",
        "  \n",
        "  # returns output of activation function\n",
        "  def apply_act_func(self, inp):\n",
        "    if(self.act_func == Constants.SIGMOID):\n",
        "      return ( 1/ ( 1+numpy.exp(-1*inp) ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eIv5_eBleba0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neuron(Neuron):\n",
        "  \n",
        "  # returns derivative of activation function\n",
        "  def apply_act_func_der(self, inp):\n",
        "    if(self.act_func == Constants.SIGMOID):\n",
        "      return (inp*(1-inp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fc24bQtkeQLv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neuron(Neuron):\n",
        "  \n",
        "  # computes output of a neuron\n",
        "  # assumption-1: outputs of start_neurons of all self.in_edges are calculated\n",
        "  # assumption-2: self.input_datum is initialized for a first-layered-neuron\n",
        "  def feed_forward(self):\n",
        "    # first-layered-neuron\n",
        "    if(self.in_edges == None): \n",
        "      self.output = self.input_datum\n",
        "      return\n",
        "    output = 0.0\n",
        "    ind = 0\n",
        "    # compute summation(w_i*x_i)\n",
        "    while(ind < len(self.in_edges)):\n",
        "      edge = self.in_edges[ind]\n",
        "      weight = edge.weight\n",
        "      inp = edge.start_neuron.output\n",
        "#       if(self.out_edges == None):\n",
        "#         print(\"edge - wi*xi: \" + str(edge) + \" - \" + str(weight) + \", \" + str(inp))\n",
        "      output = output + (weight*inp)\n",
        "      ind = ind + 1\n",
        "    # for output-layered-neurons, softmax should be applied\n",
        "    if(self.out_edges != None):\n",
        "      # apply activation function\n",
        "      self.output = self.apply_act_func(output)\n",
        "    else:\n",
        "      self.output = output\n",
        "#       print(\"summation(wi*xi): \" + str(output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "67yQsGVugbiu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neuron(Neuron):\n",
        "  \n",
        "  # back propagation (updates the weights of in_edges)\n",
        "  # local_error: before multiplying by weight\n",
        "  # assumption-1: local_errors of all end_neurons of self.out_edges are calculated\n",
        "  # assumption-2: self.output is computed\n",
        "  # assumption-3: self.loss_der is initialized for an output-layered-neuron\n",
        "  def back_prop(self):\n",
        "    # output-layered-neuron\n",
        "    if(self.out_edges == None): \n",
        "      self.local_error = self.loss_der\n",
        "    else:\n",
        "      ind = 0\n",
        "      local_error_self = 0.0\n",
        "      # compute self.local_error\n",
        "      while(ind < len(self.out_edges)):\n",
        "        edge = self.out_edges[ind]\n",
        "        weight = edge.old_weight\n",
        "        local_error_other = edge.end_neuron.local_error\n",
        "        local_error_self = local_error_self + (local_error_other * weight)\n",
        "        ind = ind + 1\n",
        "      act_func_der = self.apply_act_func_der(self.output)\n",
        "      self.local_error = local_error_self * act_func_der\n",
        "    # update self.in_edges\n",
        "    if(self.in_edges != None):\n",
        "      ind = 0\n",
        "      while(ind < len(self.in_edges)):\n",
        "        edge = self.in_edges[ind]\n",
        "        inp = edge.start_neuron.output\n",
        "        update = self.local_error * inp\n",
        "        edge.old_weight = edge.weight\n",
        "        edge.weight = edge.weight + update\n",
        "        ind = ind + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0NUqospJjTMx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# An edge between 2 neurons\n",
        "class Edge:\n",
        "  \n",
        "  id_ = None\n",
        "  start_neuron = None\n",
        "  end_neuron = None\n",
        "  weight = None\n",
        "  # old weight is required during back propagation i.e out_edges' weights are already \n",
        "  # updated, but you need old weights, Why? During back propagation, weight updations \n",
        "  # are done from right to left; at any instance during back propagation, all the \n",
        "  # weights of edges that are towards right are already updated\n",
        "  old_weight = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KrwnC7-69GNO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Edge(Edge):\n",
        "  \n",
        "  def __str__(self):\n",
        "    return str(self.start_neuron.id_ + \"->\" + self.end_neuron.id_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ICgspVoQzGhi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Fully connected Layer\n",
        "class Layer:\n",
        "  \n",
        "  # id_ = layer_num\n",
        "  id_ = None\n",
        "  tot_neurons = None\n",
        "  act_func = None\n",
        "  neurons = None\n",
        "  prev_layer = None\n",
        "  next_layer = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I-ro1iF60hiz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Layer(Layer):\n",
        "  \n",
        "  # update in_edges of cur_layer's neurons & out_edges of prev_layer's neurons\n",
        "  # assumption: should have non-None value to self.prev_layer from 2nd layer onwards\n",
        "  def build(self):\n",
        "    # cur refers to current layer\n",
        "    cur_neuron_ind = 0 \n",
        "    self.neurons = []\n",
        "    # add neurons\n",
        "    while(cur_neuron_ind < self.tot_neurons):\n",
        "      cur_neuron = Neuron()\n",
        "      cur_neuron.id_ = self.id_ + \".\" + str(cur_neuron_ind)\n",
        "      cur_neuron.act_func = self.act_func\n",
        "      # add in_edges to cur_neuron and out_edges to prev_neuron\n",
        "      if(self.prev_layer != None):\n",
        "        cur_neuron.in_edges = []\n",
        "        prev_neuron_ind = 0\n",
        "        while(prev_neuron_ind < self.prev_layer.tot_neurons):\n",
        "          prev_neuron = self.prev_layer.neurons[prev_neuron_ind]\n",
        "          if(prev_neuron.out_edges == None):\n",
        "            prev_neuron.out_edges = []\n",
        "          edge = Edge()\n",
        "          edge.id_ = str(prev_neuron.id_) + \".\" + str(len(prev_neuron.out_edges))\n",
        "          edge.start_neuron = prev_neuron\n",
        "          edge.end_neuron = cur_neuron\n",
        "          edge.weight = random.uniform(0, 1)\n",
        "          edge.weight = round(edge.weight, 1)\n",
        "          cur_neuron.in_edges.append(edge)\n",
        "          prev_neuron.out_edges.append(edge)\n",
        "          prev_neuron_ind = prev_neuron_ind + 1\n",
        "      self.neurons.append(cur_neuron)\n",
        "      cur_neuron_ind = cur_neuron_ind + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6jycN9AyhomI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Layer(Layer):\n",
        "  \n",
        "  def feed_forward(self):\n",
        "    neuron_ind = 0\n",
        "    while(neuron_ind < self.tot_neurons):\n",
        "      neuron = self.neurons[neuron_ind]\n",
        "      neuron.feed_forward()\n",
        "      neuron_ind = neuron_ind + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E4F2S58El5eL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Layer(Layer):\n",
        "  \n",
        "  def back_prop(self):\n",
        "    neuron_ind = 0\n",
        "    while(neuron_ind < self.tot_neurons):\n",
        "      neuron = self.neurons[neuron_ind]\n",
        "      neuron.back_prop()\n",
        "      neuron_ind = neuron_ind + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4mmMfHnjUnTd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neural_Network:\n",
        "  \n",
        "  # including input, hidden and output layers\n",
        "  tot_layers = None\n",
        "  # a list containing no. of neurons in each layer\n",
        "  tot_neurons = None\n",
        "  # single activation func; ex: Constants.SIGMOID\n",
        "  act_func = None\n",
        "  # a list containing actual layers\n",
        "  layers = None\n",
        "  train_x = None\n",
        "  train_y = None\n",
        "  # Index of sample that is being feed_forwarded or backpropagated\n",
        "  sample_ind = None\n",
        "  # total softmax loss after forward propagation\n",
        "  #tot_loss = None\n",
        "  tot_epochs = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IFfCfhGdVaHa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neural_Network(Neural_Network):\n",
        "  \n",
        "  def build(self):\n",
        "    cur_layer_ind = 0\n",
        "    prev_layer_ind = -1\n",
        "    self.layers = []\n",
        "    while(cur_layer_ind < self.tot_layers):\n",
        "      cur_layer = Layer()\n",
        "      cur_layer.id_ = str(cur_layer_ind)\n",
        "      cur_layer.tot_neurons = self.tot_neurons[cur_layer_ind]\n",
        "      cur_layer.act_func = self.act_func\n",
        "      if(prev_layer_ind != -1):\n",
        "        cur_layer.prev_layer = self.layers[prev_layer_ind]\n",
        "      cur_layer.build()\n",
        "      self.layers.append(cur_layer)\n",
        "      cur_layer_ind = cur_layer_ind + 1\n",
        "      prev_layer_ind = prev_layer_ind + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1tp1iYluZa1T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neural_Network(Neural_Network):\n",
        "  \n",
        "  def print(self):\n",
        "    print(\"layers: \" + str(self.tot_layers))\n",
        "    print(\"neurons: \" + str(self.tot_neurons))\n",
        "    print(\"activation: \" + str(self.act_func))\n",
        "    print(\"=======================\")\n",
        "    print(\"layer.neuron.edge: weight\")\n",
        "    layer_ind = 0\n",
        "    while(layer_ind < self.tot_layers):\n",
        "      cur_layer = self.layers[layer_ind]\n",
        "      neuron_ind = 0\n",
        "      while(neuron_ind < cur_layer.tot_neurons):\n",
        "        cur_neuron = cur_layer.neurons[neuron_ind]\n",
        "        if(cur_neuron.output != None):\n",
        "          cur_output = str(round(cur_neuron.output, 2))\n",
        "        else:\n",
        "          cur_output = \"None\"\n",
        "        if(cur_neuron.loss_der != None):\n",
        "          cur_loss_der = str(round(cur_neuron.loss_der, 2))\n",
        "        else:\n",
        "          cur_loss_der = \"None\"\n",
        "        print(cur_output + \"|\" + cur_loss_der)\n",
        "        if(cur_neuron.out_edges != None):\n",
        "          out_edge_ind = 0\n",
        "          while(out_edge_ind < len(cur_neuron.out_edges)):\n",
        "            out_edge = cur_neuron.out_edges[out_edge_ind]\n",
        "            print(str(out_edge.id_) + \": \", end=\"\")\n",
        "            print(str(round(out_edge.weight, 2)))\n",
        "            out_edge_ind = out_edge_ind + 1\n",
        "        neuron_ind = neuron_ind + 1\n",
        "      layer_ind = layer_ind + 1\n",
        "      if(layer_ind != self.tot_layers):\n",
        "        print(\"=======================\")\n",
        "    print(\"=====================================================\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CNzz_Ek_i0Dp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neural_Network(Neural_Network):\n",
        "  \n",
        "  # feeds sample to the 1st layer\n",
        "  # i.e for each neuron in the 1st layer, a feature (of a sample) should be assigned to its neuron.input_datum\n",
        "  # assumption: No. of neurons in 1st layer = no. of features in a sample\n",
        "  def feed_sample(self, sample):\n",
        "    layer_one = self.layers[0]\n",
        "    neurons = layer_one.neurons\n",
        "    neuron_ind = 0\n",
        "    while(neuron_ind < layer_one.tot_neurons):\n",
        "      neuron = neurons[neuron_ind]\n",
        "      feature = sample[neuron_ind]\n",
        "      neuron.input_datum = feature\n",
        "      neuron_ind = neuron_ind + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dhgtEEY04FPe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neural_Network(Neural_Network):\n",
        "  \n",
        "  def softmax(self, data):\n",
        "    ind = 0\n",
        "    exp_data = []\n",
        "    while(ind < len(data)):\n",
        "      cur_exp = numpy.exp(data[ind])\n",
        "      exp_data.append(cur_exp)\n",
        "      ind = ind + 1\n",
        "    sum_exp = sum(exp_data)\n",
        "    softmax_data = []\n",
        "    ind = 0\n",
        "    while(ind < len(exp_data)):\n",
        "      cur_softmax = exp_data[ind] / sum_exp\n",
        "      softmax_data.append(cur_softmax)\n",
        "      ind = ind + 1\n",
        "    return softmax_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3sF7Xs6sg9qK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neural_Network(Neural_Network):\n",
        "\n",
        "  # feeds a sample to the 1st layer\n",
        "  def feed_forward(self):\n",
        "    sample = self.train_x[self.sample_ind]\n",
        "    #print(\"input: \" + str(sample))\n",
        "    self.feed_sample(sample)\n",
        "    layer_ind = 0\n",
        "    while(layer_ind < self.tot_layers):\n",
        "      cur_layer = self.layers[layer_ind]\n",
        "      cur_layer.feed_forward()\n",
        "      layer_ind = layer_ind + 1\n",
        "    # softmax for output layer\n",
        "    output_layer = self.layers[self.tot_layers-1]\n",
        "    neuron_ind = 0\n",
        "    # output of each neuron\n",
        "    outputs = []\n",
        "    while(neuron_ind < output_layer.tot_neurons):\n",
        "      neuron = output_layer.neurons[neuron_ind]\n",
        "      cur_output = neuron.output\n",
        "      outputs.append(cur_output)\n",
        "      neuron_ind = neuron_ind + 1\n",
        "    softmax_outputs = self.softmax(outputs)\n",
        "    # modify outputs of each neuron in output layer to a softmax output\n",
        "    neuron_ind = 0\n",
        "    while(neuron_ind < output_layer.tot_neurons):\n",
        "      neuron = output_layer.neurons[neuron_ind]\n",
        "      neuron.output = softmax_outputs[neuron_ind]\n",
        "      neuron_ind = neuron_ind + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vTw6L4wq-jsS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neural_Network(Neural_Network):\n",
        "  \n",
        "  # Softmax and then Cross Entropy Loss\n",
        "  # assumption-1: class labels range between 0 and tot_classes-1\n",
        "  def compute_loss(self):\n",
        "    last_layer = self.layers[self.tot_layers-1]\n",
        "    neurons = last_layer.neurons\n",
        "    cur_label = self.train_y[self.sample_ind]\n",
        "    # derivative of loss\n",
        "    neuron_ind = 0\n",
        "    while(neuron_ind < len(neurons)):\n",
        "      neuron = neurons[neuron_ind]\n",
        "      if(neuron_ind == cur_label):\n",
        "        ground_truth_softmax = 1\n",
        "      else:\n",
        "        ground_truth_softmax = 0\n",
        "      predicted_softmax = neuron.output\n",
        "      neuron.loss_der = predicted_softmax - ground_truth_softmax\n",
        "      neuron_ind = neuron_ind + 1\n",
        "    # compute total loss (just for observing loss over iterations)\n",
        "    neuron_ind = 0\n",
        "    self.total_loss = 0.0\n",
        "    while(neuron_ind < len(neurons)):\n",
        "      softmax_output = neurons[neuron_ind].output\n",
        "      if(neuron_ind == cur_label):\n",
        "        ground_truth_softmax = 1\n",
        "      else:\n",
        "        ground_truth_softmax = 0\n",
        "      print(str(round(softmax_output, 1)) + \" | \" + str(ground_truth_softmax))\n",
        "#       softmax_loss = ground_truth_softmax * numpy.log(softmax_output)\n",
        "#       self.total_loss = self.total_loss + softmax_loss\n",
        "      neuron_ind = neuron_ind + 1\n",
        "    self.total_loss = -1 * self.total_loss\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BaBsixDOmmVP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neural_Network(Neural_Network):\n",
        "  \n",
        "  def back_prop(self):\n",
        "    layer_ind = self.tot_layers - 1\n",
        "    while(layer_ind >= 0 ):\n",
        "      layer = self.layers[layer_ind]\n",
        "      layer.back_prop()\n",
        "      layer_ind = layer_ind - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JDrEppke9o-r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neural_Network(Neural_Network):\n",
        "  \n",
        "  def train(self):\n",
        "    self.build()\n",
        "    cur_epoch = 0\n",
        "    while(cur_epoch < self.tot_epochs):\n",
        "      self.sample_ind = 0\n",
        "      while(self.sample_ind < len(self.train_x)):\n",
        "        self.feed_forward()\n",
        "        self.compute_loss()\n",
        "        #self.print()\n",
        "        self.back_prop()\n",
        "        #self.print()\n",
        "        #return\n",
        "        self.sample_ind = self.sample_ind + 1\n",
        "        #print(\"*******************sample************************\")\n",
        "      print(\"================\")\n",
        "      cur_epoch = cur_epoch + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I1ggFPg5YPQk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Main:\n",
        "  \n",
        "  @staticmethod\n",
        "  def main():\n",
        "    train_x = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "    train_y = [0, 1, 1, 0]\n",
        "    nn = Neural_Network()\n",
        "    nn.tot_layers = 3\n",
        "    nn.tot_neurons = [2, 3, 2]\n",
        "    nn.act_func = Constants.SIGMOID\n",
        "    nn.train_x = train_x\n",
        "    nn.train_y = train_y\n",
        "    nn.tot_epochs = 9\n",
        "    nn.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AxQdZg2abgHa",
        "colab_type": "code",
        "outputId": "5d1bcf56-e7df-4f58-91bb-51c827f2f2b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1411
        }
      },
      "cell_type": "code",
      "source": [
        "start_time = time()\n",
        "Main.main()\n",
        "end_time = time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(\"processed in \" + str(elapsed_time) + \" seconds.\")"
      ],
      "execution_count": 336,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5 | 1\n",
            "0.5 | 0\n",
            "0.3 | 0\n",
            "0.7 | 1\n",
            "0.6 | 0\n",
            "0.4 | 1\n",
            "0.9 | 1\n",
            "0.1 | 0\n",
            "================\n",
            "0.7 | 1\n",
            "0.3 | 0\n",
            "0.7 | 0\n",
            "0.3 | 1\n",
            "0.9 | 0\n",
            "0.1 | 1\n",
            "1.0 | 1\n",
            "0.0 | 0\n",
            "================\n",
            "1.0 | 1\n",
            "0.0 | 0\n",
            "1.0 | 0\n",
            "0.0 | 1\n",
            "1.0 | 0\n",
            "0.0 | 1\n",
            "1.0 | 1\n",
            "0.0 | 0\n",
            "================\n",
            "1.0 | 1\n",
            "0.0 | 0\n",
            "1.0 | 0\n",
            "0.0 | 1\n",
            "1.0 | 0\n",
            "0.0 | 1\n",
            "1.0 | 1\n",
            "0.0 | 0\n",
            "================\n",
            "1.0 | 1\n",
            "0.0 | 0\n",
            "1.0 | 0\n",
            "0.0 | 1\n",
            "1.0 | 0\n",
            "0.0 | 1\n",
            "1.0 | 1\n",
            "0.0 | 0\n",
            "================\n",
            "1.0 | 1\n",
            "0.0 | 0\n",
            "1.0 | 0\n",
            "0.0 | 1\n",
            "1.0 | 0\n",
            "0.0 | 1\n",
            "1.0 | 1\n",
            "0.0 | 0\n",
            "================\n",
            "1.0 | 1\n",
            "0.0 | 0\n",
            "1.0 | 0\n",
            "0.0 | 1\n",
            "1.0 | 0\n",
            "0.0 | 1\n",
            "1.0 | 1\n",
            "0.0 | 0\n",
            "================\n",
            "1.0 | 1\n",
            "0.0 | 0\n",
            "1.0 | 0\n",
            "0.0 | 1\n",
            "1.0 | 0\n",
            "0.0 | 1\n",
            "1.0 | 1\n",
            "0.0 | 0\n",
            "================\n",
            "1.0 | 1\n",
            "0.0 | 0\n",
            "1.0 | 0\n",
            "0.0 | 1\n",
            "1.0 | 0\n",
            "0.0 | 1\n",
            "1.0 | 1\n",
            "0.0 | 0\n",
            "================\n",
            "processed in 0.01789689064025879 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}