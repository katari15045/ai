{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "iqr2kbaVL_2g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Tutorial: https://blog.zhaytam.com/2018/08/15/implement-neural-network-backpropagation/\n",
        "import numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ql0j4xulb1_o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Constants:\n",
        "  \n",
        "  SIGMOID = \"sigmoid\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dGEFN5sWMFJD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "  \n",
        "  # total no. of inputs to the layer\n",
        "  tot_inputs = None\n",
        "  # an integer\n",
        "  # total neurons in the layer\n",
        "  tot_neurons = None\n",
        "  # 2d matrix of floats; size = tot_inputs x tot_neurons\n",
        "  weights = None\n",
        "  # string; activation function for each neuron in the layer\n",
        "  act_func = None\n",
        "  # 1d array of floats; each element is the bias of a neuron; size = no. of neurons\n",
        "  biases = None\n",
        "  # 1d array of floats -\n",
        "  # for input_layer, each element is a feature of a sample or data_point; \n",
        "  # for, hidden_layer, each element is the output of a neuron in the previous layer\n",
        "  inputs = None\n",
        "  # 1d array; each element is the output of a neuron\n",
        "  outputs = None\n",
        "  # error that flows to other layers from current layer during back propagation\n",
        "  error_out = None\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AlGz6UqiTCsa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Layer(Layer):\n",
        "  \n",
        "  def __init__(self, tot_inputs, tot_neurons, act_func=Constants.SIGMOID):\n",
        "    self.tot_inputs = tot_inputs\n",
        "    self.tot_neurons = tot_neurons\n",
        "    self.act_func = act_func\n",
        "    # generate random floats in range [0, 1)\n",
        "    self.weights = numpy.random.rand(self.tot_inputs, self.tot_neurons)\n",
        "    # generate random numbers in range [0, 1)\n",
        "    self.biases = numpy.random.rand(self.tot_neurons)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Ow9aub5bsPP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Layer(Layer):\n",
        "  \n",
        "  def apply_act_func(self, x):\n",
        "    if(self.act_func == Constants.SIGMOID):\n",
        "      return 1/(1+numpy.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fU1KJC7ckl6f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Layer(Layer):\n",
        "  \n",
        "  # returns derivative of activation function\n",
        "  def apply_act_der(self, x):\n",
        "    if(self.act_func == Constants.SIGMOID):\n",
        "      return x*(1-x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E0YSrIrcXabS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Layer(Layer):\n",
        "  \n",
        "  def feed_forward(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    before_act_func = numpy.dot(inputs, self.weights) + self.biases\n",
        "    self.outputs = self.apply_act_func(before_act_func)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N5borj2FU3hT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "  \n",
        "  # 1d array; each element is of type Layer\n",
        "  layers = None\n",
        "  # 2d array; each element is a sample or data_point\n",
        "  train_x = None\n",
        "  # 1d array; each element is a label for respective sample in train_x\n",
        "  train_y = None\n",
        "  # integer; Total number of epochs.\n",
        "  tot_epochs = None\n",
        "  # learning rate during weight updation\n",
        "  learning_rate = None\n",
        "  # 1d array; each element is output of neuron in output layer after feed forward\n",
        "  outputs = None\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lcrPkESaWLkG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(NeuralNetwork):\n",
        "  \n",
        "  def __init__(self, train_x, train_y, tot_epochs, learning_rate):\n",
        "    self.train_x = train_x\n",
        "    self.train_y = numpy.reshape(train_y, (len(train_y), 1))\n",
        "    self.tot_epochs = tot_epochs\n",
        "    self.learning_rate = learning_rate\n",
        "    self.layers = []\n",
        "    self.tot_inputs_to_new_layer = train_x.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j2hA5NF_Vaj_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(NeuralNetwork):\n",
        "  \n",
        "  def add_layer(self, tot_neurons, act_func):\n",
        "    layer = Layer(self.tot_inputs_to_new_layer, tot_neurons, act_func)\n",
        "    self.layers.append(layer)\n",
        "    self.tot_inputs_to_new_layer = tot_neurons\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kkO_ffE9WqMQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(NeuralNetwork):\n",
        "  \n",
        "  # data: 1d array; a training sample; each element is a feature\n",
        "  def feed_forward(self, data):\n",
        "    for layer in self.layers:\n",
        "      layer.feed_forward(data)\n",
        "      data = layer.outputs\n",
        "    self.outputs = data\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yK6TIXACeNVv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(NeuralNetwork):\n",
        "  \n",
        "  # assumption: feed_forward is completed\n",
        "  def back_prop(self, sample, ground_truth):\n",
        "    output_layer_ind = len(self.layers) - 1\n",
        "    cur_layer_ind = output_layer_ind\n",
        "    while(cur_layer_ind >= 0):\n",
        "      cur_layer = self.layers[cur_layer_ind]\n",
        "      if(cur_layer_ind == output_layer_ind):\n",
        "        # error_in = incoming error = derivative of Mean Squared Error\n",
        "        prediction = cur_layer.outputs\n",
        "        error_in = ground_truth - prediction\n",
        "      else:\n",
        "        # not the output_layer; a hidden layer.\n",
        "        next_layer = self.layers[cur_layer_ind+1]\n",
        "        # error_in = incoming error = weighted error that floats from the next_layer to current_layer\n",
        "        error_in = numpy.dot(next_layer.weights, next_layer.error_out)\n",
        "      # error_out = outcoming error = error that flows to other (left layers) layers from current_layer\n",
        "      cur_layer.error_out = error_in * cur_layer.apply_act_der(cur_layer.outputs)\n",
        "      cur_layer_ind = cur_layer_ind - 1\n",
        "    # update weights\n",
        "    cur_layer_ind = 0\n",
        "    while(cur_layer_ind <= output_layer_ind):\n",
        "      cur_layer = self.layers[cur_layer_ind]\n",
        "      if(cur_layer_ind == 0):\n",
        "        inp_to_cur_layer = sample\n",
        "      else:\n",
        "        # not the 1st hidden layer.\n",
        "        prev_layer = self.layers[cur_layer_ind-1]\n",
        "        inp_to_cur_layer = prev_layer.outputs\n",
        "      # we need a 2d matrix; weights is a 2d matrix\n",
        "      inp_to_cur_layer = numpy.atleast_2d(inp_to_cur_layer)\n",
        "      # error that's used for updating weights\n",
        "      error_upd = cur_layer.error_out * inp_to_cur_layer.T\n",
        "      cur_layer.weights = cur_layer.weights + (self.learning_rate * error_upd)\n",
        "      cur_layer_ind = cur_layer_ind + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_W7gr6p8wWVU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(NeuralNetwork):\n",
        "  \n",
        "  def train(self):\n",
        "    cur_epoch = 0\n",
        "    while(cur_epoch < self.tot_epochs):\n",
        "      cur_sample_ind = 0\n",
        "      while(cur_sample_ind < len(self.train_x)):\n",
        "        cur_sample = self.train_x[cur_sample_ind]\n",
        "        cur_label = self.train_y[cur_sample_ind]\n",
        "        self.feed_forward(cur_sample)\n",
        "        self.back_prop(cur_sample, cur_label)\n",
        "        cur_sample_ind = cur_sample_ind + 1\n",
        "      # compute error on whole train_x i.e all samples for logging\n",
        "      self.feed_forward(self.train_x)\n",
        "      last_layer = self.layers[-1]\n",
        "      mse = numpy.mean(numpy.square(self.train_y - last_layer.outputs))\n",
        "      print(\"epoch: \" + str(cur_epoch+1) + \", mse: \" + str(mse))\n",
        "      cur_epoch = cur_epoch + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fLDQZL14yT9V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Main:\n",
        "  \n",
        "  @staticmethod\n",
        "  def main():\n",
        "    train_x = numpy.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    train_y = numpy.array([0, 1, 1, 0])\n",
        "    tot_epochs = 9\n",
        "    learning_rate = 0.3\n",
        "    nn = NeuralNetwork(train_x, train_y, tot_epochs, learning_rate)\n",
        "    nn.add_layer(3, Constants.SIGMOID)\n",
        "    nn.add_layer(2, Constants.SIGMOID)\n",
        "    nn.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r3xo6pT8zBS9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "62b8ff4e-ac34-4592-a7df-9ceb2d23e676"
      },
      "cell_type": "code",
      "source": [
        "Main.main()"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, mse: 0.39494288853401827\n",
            "epoch: 2, mse: 0.389534617646196\n",
            "epoch: 3, mse: 0.383873257365011\n",
            "epoch: 4, mse: 0.3779755426385345\n",
            "epoch: 5, mse: 0.37186492964758555\n",
            "epoch: 6, mse: 0.3655716541453815\n",
            "epoch: 7, mse: 0.3591323576072299\n",
            "epoch: 8, mse: 0.352589253388238\n",
            "epoch: 9, mse: 0.3459888720520628\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}